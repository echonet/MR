{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from utils import bootstrap\n",
    "from \n",
    "from tqdm import tqdm\n",
    "from matplotlib import rc\n",
    "\n",
    "def bootstrap_ppv_f1_recall(x,y):\n",
    "    y_total,yhat_total = x,y\n",
    "    ppv_list = []\n",
    "    f1_score_list = []\n",
    "    recall_list = []\n",
    "    \n",
    "    # bootstrap for confidence interval\n",
    "    for i in tqdm(range(0,10000)):\n",
    "        choices = np.random.choice(range(0,len(yhat_total)),int(len(yhat_total)/2))\n",
    "        ground = y_total[choices]\n",
    "        ground.index = range(0,len(ground))\n",
    "        preds = yhat_total[choices]\n",
    "        preds.index = range(0,len(preds))\n",
    "        ppv_list.append(metrics.precision_score(ground, preds))\n",
    "        f1_score_list.append(metrics.f1_score(ground, preds))\n",
    "        recall_list.append(metrics.recall_score(ground, preds))\n",
    "        \n",
    "    lower_point_ppv = round(np.percentile(ppv_list,2.5),3)\n",
    "    higher_point_ppv = round(np.percentile(ppv_list,97.5),3)\n",
    "    \n",
    "    lower_point_f1 = round(np.percentile(f1_score_list,2.5),3)\n",
    "    higher_point_f1 = round(np.percentile(f1_score_list,97.5),3)\n",
    "    \n",
    "    lower_point_recall = round(np.percentile(recall_list,2.5),3)\n",
    "    higher_point_recall = round(np.percentile(recall_list,97.5),3)\n",
    "\n",
    "    \n",
    "    ppv_preds = [lower_point_ppv, higher_point_ppv]\n",
    "    f1_preds = [lower_point_f1, higher_point_f1]\n",
    "    recall_preds = [lower_point_recall, higher_point_recall]\n",
    "    \n",
    "    print('PPV is ' + str(ppv_preds))\n",
    "    print('F1 is ' + str(f1_preds))\n",
    "    print('Recall is ' + str(recall_preds))\n",
    "\n",
    "test_predictions = pd.read_csv()\n",
    "test_predictions = test_predictions.drop_duplicates('filename')\n",
    "test_predictions.index = range(0,len(test_predictions))\n",
    "\n",
    "\n",
    "cols = ['Control_preds','Mild_preds','Moderate_preds','Severe_preds']\n",
    "\n",
    "for i in cols:\n",
    "    test_predictions[i] = test_predictions[i].apply(sigmoid)\n",
    "\n",
    "test_predictions.final_class.value_counts()\n",
    "\n",
    "test_predictions['predicted'] = test_predictions[cols].idxmax(axis = 1).astype(str).str.slice(stop = -6)\n",
    "\n",
    "test_predictions.final_class.value_counts()\n",
    "\n",
    "min_number = pd.DataFrame(test_predictions.final_class.value_counts()).final_class[-1]\n",
    "ordered_indices = ['Control','Mild','Moderate','Severe']\n",
    "\n",
    "cm = metrics.confusion_matrix(test_predictions['final_class'], test_predictions['predicted'])\n",
    "\n",
    "cm = pd.DataFrame(cm, columns = np.sort(test_predictions.final_class.unique()),\n",
    "                  index = np.sort(test_predictions.final_class.unique()))\n",
    "\n",
    "\n",
    "fpr_severe, tpr_severe, thresholds = metrics.roc_curve((test_predictions.final_class == 'Severe') * 1, \n",
    "                                         test_predictions.Severe_preds)\n",
    "fpr_mod_severe, tpr_mod_severe, thresholds = metrics.roc_curve((test_predictions.final_class.isin(\n",
    "    ['Moderate','Severe']) * 1), test_predictions[['Moderate_preds','Severe_preds']].max(axis = 1))\n",
    "\n",
    "scaling_factor = 1.5\n",
    "\n",
    "plt.figure(figsize=(8*scaling_factor,8*scaling_factor))\n",
    "\n",
    "lw = 2*scaling_factor\n",
    "ls = 'dashed'\n",
    "stanford_color = 'C0'\n",
    "cedars_color = 'C2'\n",
    "\n",
    "## Make severe a solid line\n",
    "## Make moderate to severe a dashed line\n",
    "\n",
    "plt.plot(fpr_severe, tpr_severe, linewidth = lw, color = cedars_color, label = ('Severe - AUC: ' + str(round(metrics.auc(fpr_severe, tpr_severe),3))))\n",
    "plt.plot(fpr_mod_severe, tpr_mod_severe, linestyle = ls, linewidth = lw, color = cedars_color, label = 'â‰¥ Moderate - AUC: ' + str(round(metrics.auc(fpr_mod_severe, tpr_mod_severe),4))[:-1])\n",
    "plt.xlabel('1- Specificity', fontsize = 20*scaling_factor, rotation = 0, labelpad=10)\n",
    "plt.ylabel('Sensitivity', fontsize = 20*scaling_factor, rotation = 90, labelpad=15)\n",
    "# plt.title('MR Detection - Severity and Institution',fontsize = 20)\n",
    "plt.xticks(fontsize = 17*scaling_factor)\n",
    "plt.yticks(fontsize = 17*scaling_factor)\n",
    "\n",
    "legend = plt.legend(title = \"Severity and Institution\", fontsize = 15*1.5)\n",
    "plt.setp(legend.get_title(),fontsize= 17*1.5)\n",
    "# ax.set_ylabel('abc', rotation=0, fontsize=20, labelpad=20)\n",
    "\n",
    "\n",
    "# plt.plot(, linestyle = 'dashed', color = 'red')\n",
    "# plt.plot(, linestyle = 'dashed', color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols = ['Control_preds','Mild_preds','Moderate_preds','Severe_preds']\n",
    "\n",
    "for i in cols:\n",
    "    test_predictions[i] = test_predictions[i].apply(sigmoid)\n",
    "\n",
    "test_predictions.final_class.value_counts()\n",
    "\n",
    "test_predictions['predicted'] = test_predictions[cols].idxmax(axis = 1).astype(str).str.slice(stop = -6)\n",
    "\n",
    "test_predictions.final_class.value_counts()\n",
    "\n",
    "min_number = pd.DataFrame(test_predictions.final_class.value_counts()).final_class[-1]\n",
    "ordered_indices = ['Control','Mild','Moderate','Severe']\n",
    "\n",
    "# control_index = test_predictions[test_predictions.final_class.isin(['Control'])].sample(n = min_number).index\n",
    "# mild_mr_index = test_predictions[test_predictions.final_class.isin(['Mild'])].sample(n = min_number).index\n",
    "# moderate_mr_index = test_predictions[test_predictions.final_class.isin(['Moderate'])].sample(n = min_number).index\n",
    "# severe_mr_index = test_predictions[test_predictions.final_class.isin(['Severe'])].index\n",
    "# indices = list(control_index) + list(mild_mr_index) + list(moderate_mr_index) + list(severe_mr_index)\n",
    "# test_predictions = test_predictions[test_predictions.index.isin(indices)]\n",
    "\n",
    "cm = metrics.confusion_matrix(test_predictions['final_class'], test_predictions['predicted'])\n",
    "\n",
    "cm = pd.DataFrame(cm, columns = np.sort(test_predictions.final_class.unique()),\n",
    "                  index = np.sort(test_predictions.final_class.unique()))\n",
    "# cm = cm[ordered_indices].loc[ordered_indices]\n",
    "# cm_original = cm\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "# cm_try = cm/cm.sum(axis = 1)\n",
    "cm_try = cm.copy()\n",
    "cm_try.loc['Control'] = cm.loc['Control']/cm.loc['Control'].sum()\n",
    "cm_try.loc['Mild'] = cm.loc['Mild']/cm.loc['Mild'].sum()\n",
    "cm_try.loc['Moderate'] = cm.loc['Moderate']/cm.loc['Moderate'].sum()\n",
    "cm_try.loc['Severe'] = cm.loc['Severe']/cm.loc['Severe'].sum()\n",
    "\n",
    "cm_try = cm_try.rename(columns = {'Control':'None'}, index = {'Control':'None'})\n",
    "\n",
    "res = sns.heatmap(cm_try, annot=cm.to_numpy(), cmap='Greens', linewidths = 5, linecolor='black',fmt=',d',\n",
    "           vmin = 0, vmax = cm_try.to_numpy().max(), cbar = False, square = True)   \n",
    "    \n",
    "# Drawing the frame \n",
    "res.axhline(y = 0, color='k',linewidth = 10) \n",
    "res.axhline(y = cm.shape[1], color = 'k', \n",
    "            linewidth = 10) \n",
    "  \n",
    "res.axvline(x = 0, color = 'k', \n",
    "            linewidth = 10) \n",
    "  \n",
    "res.axvline(x = cm.shape[0],  \n",
    "            color = 'k', linewidth = 10) \n",
    "  \n",
    "# show plot \n",
    "plt.title('Mitral Regurgitation Classification - CSMC\\n', fontsize = 25, fontweight = 'bold')\n",
    "plt.ylabel('Actual  ', fontsize = 23, rotation = 0, fontweight = 'bold')\n",
    "plt.xlabel('\\nPredicted', fontsize = 23, fontweight = 'bold')\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.yticks(fontsize = 20, rotation = 0)\n",
    "\n",
    "# for _, spine in res.spines.items(): \n",
    "#     spine.set_visible(True) \n",
    "#     spine.set_linewidth(5)\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "\n",
    "test_predictions['severe_binary'] = (test_predictions['final_class'].isin(['Severe'])*1)\n",
    "test_predictions['severe_binary_pred'] = (test_predictions['predicted'].isin(['Severe'])*1)\n",
    "test_predictions['mod_severe_binary'] = (test_predictions['final_class'].isin(['Moderate','Severe'])*1)\n",
    "test_predictions['mod_severe_binary_pred'] = (test_predictions['predicted'].isin(['Moderate','Severe'])*1)\n",
    "test_predictions['control_mild_binary'] = (test_predictions['final_class'].isin(['Control','Mild'])*1)\n",
    "test_predictions['control_mild_binary_pred'] = (test_predictions['predicted'].isin(['Control','Mild'])*1)\n",
    "test_predictions['moderate_binary'] = (test_predictions['final_class'].isin(['Moderate'])*1)\n",
    "test_predictions['moderate_binary_pred'] = (test_predictions['predicted'].isin(['Moderate'])*1)\n",
    "\n",
    "test_predictions['not_severe_binary'] = (~test_predictions['final_class'].isin(['Severe'])*1)\n",
    "test_predictions['not_severe_binary_pred'] = (~test_predictions['predicted'].isin(['Severe'])*1)\n",
    "\n",
    "test_predictions['Mod_Severe_preds'] = test_predictions[\n",
    "    ['Moderate_preds','Severe_preds']].max(axis = 1, skipna = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Severe\n",
    "\n",
    "print('Severe PPV is ' + str(round(metrics.precision_score(test_predictions['severe_binary'], test_predictions['severe_binary_pred'],\n",
    "                       labels = ['Severe']),3)))\n",
    "\n",
    "print('Severe NPV is ' + str(round(metrics.precision_score(test_predictions['not_severe_binary'], test_predictions['not_severe_binary_pred'],\n",
    "                       labels = ['Severe']),3)))\n",
    "\n",
    "print('Severe Recall is ' + str(round(metrics.recall_score(test_predictions['severe_binary'], test_predictions['severe_binary_pred'],\n",
    "                       labels = ['Severe']),3)))\n",
    "\n",
    "print('Severe F1-Score is ' + str(round(metrics.f1_score(test_predictions['severe_binary'], test_predictions['severe_binary_pred'],\n",
    "                       labels = ['Severe']),3)) + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "### Moderate/Severe\n",
    "\n",
    "print('Moderate/Severe PPV is ' + str(round(metrics.precision_score(test_predictions['mod_severe_binary'], test_predictions['mod_severe_binary_pred'],\n",
    "                       labels = ['Moderate/Severe']),3)))\n",
    "\n",
    "\n",
    "print('Moderate/Severe NPV is ' + str(round(metrics.precision_score(test_predictions['control_mild_binary'], test_predictions['control_mild_binary_pred'],\n",
    "                       labels = ['Moderate/Severe']),3)))\n",
    "\n",
    "print('Moderate/Severe Recall is ' + str(round(metrics.recall_score(test_predictions['mod_severe_binary'], test_predictions['mod_severe_binary_pred'],\n",
    "                       labels = ['Moderate/Severe']),3)))\n",
    "\n",
    "print('Moderate/Severe F1-Score is ' + str(round(metrics.f1_score(test_predictions['mod_severe_binary'], test_predictions['mod_severe_binary_pred'],\n",
    "                       labels = ['Moderate/Severe']),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
